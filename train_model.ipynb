{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11913222,"sourceType":"datasetVersion","datasetId":7489628}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a9023cdc-13b2-4086-80a1-e63f8acbd51d","cell_type":"code","source":"!pip install torch transformers scikit-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e39ec47a-fcbe-4630-b9af-7e9727ddde17","cell_type":"code","source":"import torch\nfrom transformers import (\n    T5ForConditionalGeneration, \n    T5Tokenizer, \n    Trainer, \n    TrainingArguments,\n    DataCollatorForSeq2Seq\n)\nfrom torch.utils.data import Dataset\nimport json\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom collections import Counter\n\n# Check GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nclass SnortRuleDataset(Dataset):\n    def __init__(self, data, tokenizer, max_input_length=128, max_target_length=256):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_input_length = max_input_length\n        self.max_target_length = max_target_length\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        input_text = f\"Generate Snort rule: {item['input'].strip().lower()}\"\n        target_text = item['target']\n\n        input_encoding = self.tokenizer(\n            input_text,\n            max_length=self.max_input_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        target_encoding = self.tokenizer(\n            target_text,\n            max_length=self.max_target_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        return {\n            \"input_ids\": input_encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": input_encoding[\"attention_mask\"].flatten(),\n            \"labels\": target_encoding[\"input_ids\"].flatten()\n        }\n\nwith open(\"/kaggle/working/new_dataset.json\", \"r\") as ddd:\n    training_data = json.load(ddd)\n\ndef replicate_critical_pairs(data, min_count=10):\n    counter = Counter([item['input'] for item in data])\n    amplified = []\n    for item in data:\n        repeat = max(min_count - counter[item['input']], 1)\n        amplified.extend([item] * repeat)\n    return amplified\n\nclass SnortRuleGenerator:\n    def __init__(self, model_name=\"t5-small\"):\n        self.model_name = model_name\n        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n        self.device = device\n        self.model.to(self.device)\n\n        special_tokens = [\"<snort>\", \"<rule>\", \"<alert>\"]\n        self.tokenizer.add_tokens(special_tokens)\n        self.model.resize_token_embeddings(len(self.tokenizer))\n\n    def prepare_data(self, data, test_size=0.3):\n        data = replicate_critical_pairs(data)\n        train_data, val_data = train_test_split(data, test_size=test_size, random_state=42)\n        train_dataset = SnortRuleDataset(train_data, self.tokenizer)\n        val_dataset = SnortRuleDataset(val_data, self.tokenizer)\n        return train_dataset, val_dataset\n\n    def train(self, train_dataset, val_dataset, output_dir=\"./snort-rule-model\", num_epochs=10, batch_size=8, learning_rate=5e-5):\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=num_epochs,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=batch_size,\n            warmup_steps=50,\n            weight_decay=0.01,\n            logging_dir=f'{output_dir}/logs',\n            logging_steps=10,\n            evaluation_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            save_total_limit=2,\n            load_best_model_at_end=True,\n            metric_for_best_model=\"eval_loss\",\n            greater_is_better=False,\n            learning_rate=learning_rate,\n            fp16=True,\n            dataloader_pin_memory=True,\n            gradient_accumulation_steps=1\n        )\n\n        data_collator = DataCollatorForSeq2Seq(\n            tokenizer=self.tokenizer,\n            model=self.model,\n            padding=True\n        )\n\n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=val_dataset,\n            data_collator=data_collator,\n            tokenizer=self.tokenizer,\n        )\n\n        print(\"Starting training...\")\n        trainer.train()\n        trainer.save_model()\n        self.tokenizer.save_pretrained(output_dir)\n        print(f\"Model saved to {output_dir}\")\n\n    def generate_rule(self, input_text, max_length=256, num_beams=1, temperature=0.0):\n        input_text = f\"Generate Snort rule: {input_text.strip().lower()}\"\n        input_ids = self.tokenizer.encode(\n            input_text,\n            return_tensors=\"pt\",\n            max_length=128,\n            truncation=True\n        ).to(self.device)\n\n        with torch.no_grad():\n            outputs = self.model.generate(\n                input_ids,\n                max_length=max_length,\n                num_beams=num_beams,\n                temperature=temperature,\n                do_sample=False,\n                pad_token_id=self.tokenizer.pad_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n                early_stopping=True\n            )\n        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    def load_model(self, model_path):\n        self.model = T5ForConditionalGeneration.from_pretrained(model_path)\n        self.tokenizer = T5Tokenizer.from_pretrained(model_path)\n        self.model.to(self.device)\n        print(f\"Model loaded from {model_path}\")\n\ndef main():\n    # Initialize generator\n    generator = SnortRuleGenerator(\"t5-small\")\n    \n    # Prepare datasets\n    train_dataset, val_dataset = generator.prepare_data(training_data)\n    \n    # Train the model\n    generator.train(\n        train_dataset, \n        val_dataset, \n        num_epochs=15,  # Increase for better results\n        batch_size=4,   # Adjust based on your GPU memory\n        learning_rate=3e-4\n    )\n    return\n\ndef inference_example():\n    \"\"\"Example of using the trained model for inference\"\"\"\n    generator = SnortRuleGenerator()\n    \n    # Load your trained model\n    generator.load_model(\"./snort-rule-model\")\n    \n    # Interactive generation\n    while True:\n        user_input = input(\"\\nEnter security rule description (or 'quit' to exit): \")\n        if user_input.lower() == 'quit':\n            break\n        \n        rule = generator.generate_rule(user_input)\n        print(f\"Generated Snort Rule: {rule}\")\n\nif __name__ == \"__main__\":\n    # Run training\n    main()\n    \n    # Uncomment for interactive inference\n    # inference_example()\n\n# Additional utility functions for data preprocessing\ndef load_data_from_json(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\n\ndef augment_data(original_data):\n    \"\"\"Simple data augmentation techniques\"\"\"\n    augmented = []\n    \n    synonyms = {\n    \"detect\": [\"monitor\", \"inspect\", \"capture\", \"check\", \"catch\", \"track\"],\n    \"bidirectional\": [\"in both directions\", \"two-way\", \"both ways\"],\n    \"ICMP\": [\"ping\", \"echo\", \"ICMP packets\"],\n    \"HTTP GET\": [\"web GET\", \"GET method\", \"GET requests\", \"port 80 GET\"],\n    \"HTTP POST\": [\"form submission\"],\n    \"SSH brute force\": [\"SSH password cracking\", \"SSH dictionary\", \"SSH login attempts\", \"SSH authentication\", \"SSH credential stuffing\", \"SSH password guessing\"],\n    \"FTP file uploads\": [\"FTP STOR command\", \"FTP uploads\", \"FTP upload actions\", \"FTP file send operations\"],\n    \"FTP file downloads\": [\"FTP RETR command\"],\n    \"DNS tunneling\": [\"DNS exfiltration\", \"large DNS packets\", \"suspicious port 53\", \"DNS covert channel\", \"abnormal DNS query sizes\"],\n    \"SQL injection\": [\"SQL attacks\", \"UNION SELECT\", \"SQLi\", \"database injection\", \"blind SQL\", \"time-based SQL\", \"boolean-based SQL\", \"order-by injections\"],\n    \"XSS\": [\"cross-site scripting\", \"script injections\", \"JavaScript injection\", \"web code injection\", \"DOM-based XSS\", \"reflected XSS\", \"stored XSS\", \"XSS filter bypass\"],\n    \"port scan\": [\"network scanning\", \"SYN scan\", \"open port scanning\", \"reconnaissance\", \"connect scan\", \"stealth scan\", \"UDP scanning\", \"nmap scanning\"],\n    \"HTTPS anomalies\": [\"SSL traffic\", \"TLS traffic\", \"encrypted web traffic\", \"SSL certificate issues\", \"TLS handshake anomalies\"],\n    \"DoS\": [\"denial of service\", \"SYN flood\", \"server overload\"],\n    \"DDoS\": [\"distributed DoS\", \"multi-source DoS\", \"amplification attacks\", \"volumetric DDoS\"],\n    \"malware C2\": [\"command and control\", \"botnet C2\", \"trojan C&C\", \"beacon traffic\", \"heartbeat signals\", \"callback communication\"],\n    \"ransomware\": [\"file locking\", \"cryptovirus\", \"encrypted file\", \"ransomware note\", \"WannaCry\", \"Locky\", \"CryptoLocker\"],\n    \"cryptojacking\": [\"crypto mining\", \"bitcoin mining\", \"coinhive\", \"browser-based mining\", \"Monero\", \"WebAssembly\", \"CPU mining\"],\n    \"IoT botnet\": [\"smart device botnet\", \"embedded system threats\", \"Mirai\"],\n    \"phishing\": [\"fake login\", \"phishing page\", \"fraudulent login\", \"social engineering\", \"PayPal phishing\", \"bank phishing\", \"Microsoft phishing\"],\n    \"web shell\": [\"PHP shell\", \"ASP shell\", \"JSP shell\", \"remote code\", \"malicious upload\", \"backdoor script\"],\n    \"lateral movement\": [\"pivot\", \"admin share\", \"WMI\", \"PsExec\", \"internal spread\", \"remote access\"],\n    \"privilege escalation\": [\"root access\", \"admin gain\", \"superuser access\", \"UAC bypass\", \"Windows escalation\", \"kernel exploit\"],\n    \"data exfiltration\": [\"data leaks\", \"info theft\", \"FTP exfil\", \"HTTP exfil\", \"email leaks\"],\n    \"keylogger\": [\"keystroke theft\", \"keyboard logger\", \"screen capture\", \"mouse tracking\", \"credential theft\"],\n    \"steganography\": [\"hidden data\", \"image hiding\", \"large JPEG\", \"PNG\", \"MP3\"],\n    \"MITM\": [\"man-in-the-middle\", \"SSL spoofing\", \"SSL stripping\", \"cert pinning bypass\", \"DNS poisoning\"],\n    \"DNS poisoning\": [\"DNS spoofing\", \"domain poisoning\", \"DNS hijacking\", \"pharming\"],\n    \"ARP spoofing\": [\"ARP poisoning\", \"MAC spoofing\", \"Ethernet spoofing\", \"network poisoning\", \"gratuitous ARP\", \"ARP flood\"],\n    \"VLAN hopping\": [\"double tagging\"],\n    \"command injection\": [\"OS command\", \"system command\", \"shell injection\", \"Windows command\", \"PowerShell\", \"blind injection\"],\n    \"XXE\": [\"XML external entity\", \"XML parser\", \"XML injection\", \"entity expansion\", \"billion laughs\"],\n    \"directory traversal\": [\"path traversal\", \"filesystem access\", \"dot-dot-slash\", \"upper directory\", \"Windows traversal\"],\n    \"credential harvesting\": [\"auth data theft\", \"login harvesting\", \"credit card theft\", \"SSN leaks\"],\n    \"RAT\": [\"remote access trojan\", \"NetBus\", \"SubSeven\", \"Back Orifice\"],\n    \"MITM cert\": [\"self-signed\", \"cert spoof\", \"SSL stripping\"]\n}\n\n    for item in original_data:\n        augmented.append(item)\n        \n        # Create variations\n        input_text = item[\"input\"]\n        for original, alternatives in synonyms.items():\n            if original in input_text:\n                for alt in alternatives:\n                    new_input = input_text.replace(original, alt)\n                    augmented.append({\n                        \"input\": new_input,\n                        \"target\": item[\"target\"]\n                    })\n    \n    return augmented\n\n\nprint(\"DONE!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T21:28:20.668341Z","iopub.execute_input":"2025-05-22T21:28:20.668607Z","iopub.status.idle":"2025-05-22T21:42:59.030040Z","shell.execute_reply.started":"2025-05-22T21:28:20.668589Z","shell.execute_reply":"2025-05-22T21:42:59.029235Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3525' max='3525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3525/3525 14:36, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.137800</td>\n      <td>0.106465</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.077900</td>\n      <td>0.049047</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.047300</td>\n      <td>0.026680</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.031100</td>\n      <td>0.014034</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.020900</td>\n      <td>0.007685</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.013700</td>\n      <td>0.003761</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.008000</td>\n      <td>0.001919</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.006400</td>\n      <td>0.001204</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.005900</td>\n      <td>0.000577</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.005100</td>\n      <td>0.000451</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.002600</td>\n      <td>0.000399</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.002500</td>\n      <td>0.000287</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.002900</td>\n      <td>0.000216</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.002200</td>\n      <td>0.000176</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.002400</td>\n      <td>0.000165</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"name":"stdout","text":"Model saved to ./snort-rule-model\nSetup complete! Run main() to start training or use the SnortRuleGenerator class directly.\n","output_type":"stream"}],"execution_count":55},{"id":"9456fbe4-3ac6-4959-843d-6cce28bfce5e","cell_type":"code","source":"def test_single_input(model_path, input_text):\n    generator = SnortRuleGenerator()\n    generator.load_model(model_path)\n\n    result = generator.generate_rule(input_text)\n    return result\n    print(f\"\\nInput: {input_text}\")\n    print(f\"Generated Snort Rule: {result}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T21:44:09.377517Z","iopub.execute_input":"2025-05-22T21:44:09.378236Z","iopub.status.idle":"2025-05-22T21:44:11.132772Z","shell.execute_reply.started":"2025-05-22T21:44:09.378210Z","shell.execute_reply":"2025-05-22T21:44:11.132003Z"}},"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Model loaded from ./snort-rule-model\n\nInput: catch gratuitous ARP\nGenerated Snort Rule: alert arp any any -> any any (msg:\"ARP Spoofing Attack\"; threshold:type threshold, track by_src, count 5, seconds 60; sid:1000015; rev:1;)\n","output_type":"stream"}],"execution_count":57},{"id":"aca5bb32-5b50-4dcc-bad1-9e1ab1a9283a","cell_type":"code","source":"with open(\"/kaggle/working/tr_eng.txt\", \"r\", encoding=\"utf-8\") as tr_eng:\n    obj_list = [el.split(\",\") for el in tr_eng.readlines()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:05:52.683480Z","iopub.execute_input":"2025-05-22T22:05:52.684092Z","iopub.status.idle":"2025-05-22T22:05:52.688561Z","shell.execute_reply.started":"2025-05-22T22:05:52.684055Z","shell.execute_reply":"2025-05-22T22:05:52.688016Z"}},"outputs":[],"execution_count":70},{"id":"e635e5fc-7228-4d56-bba4-00ac461fc46a","cell_type":"code","source":"with open(\"tr_eng.txt\", \"r\", encoding=\"utf-8\") as tr_eng:\n    obj_list = [el.split(\",\") for el in tr_eng.readlines()]\ndef find_from_inputs(input:str):\n    for el in obj_list:\n        if input.lower() == el[0].lower():\n            return el[1]\n\ntest_single_input(\"./snort-rule-model\", find_from_inputs(\"web sitesi GET taleplerini tespit et\"))\ntest_single_input(\"./snort-rule-model\", find_from_inputs(user_input))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T22:06:17.260534Z","iopub.execute_input":"2025-05-22T22:06:17.260836Z","iopub.status.idle":"2025-05-22T22:06:18.975361Z","shell.execute_reply.started":"2025-05-22T22:06:17.260817Z","shell.execute_reply":"2025-05-22T22:06:18.974745Z"}},"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Model loaded from ./snort-rule-model\n\nInput: detect web GET requests\n\nGenerated Snort Rule: alert tcp any any -> any 80 (msg:\"HTTP GET Request\"; content:\"GET\"; http_method; sid:1000002; rev:1;)\n","output_type":"stream"}],"execution_count":72}]}